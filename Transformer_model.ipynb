{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MS Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip uninstall scikit-learn -y\n",
    "!pip uninstall imbalanced-learn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install scikit-learn\n",
    "#imbalanced-learn==0.10.1 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --user imbalanced-learn==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn==1.4.2 imbalanced-learn==0.12.0 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:35:39.058718Z",
     "iopub.status.busy": "2025-04-21T15:35:39.058340Z",
     "iopub.status.idle": "2025-04-21T15:35:52.755146Z",
     "shell.execute_reply": "2025-04-21T15:35:52.754135Z",
     "shell.execute_reply.started": "2025-04-21T15:35:39.058693Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_object_dtype, is_integer_dtype, is_float_dtype\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:36:33.291406Z",
     "iopub.status.busy": "2025-04-21T15:36:33.290730Z",
     "iopub.status.idle": "2025-04-21T15:36:33.303263Z",
     "shell.execute_reply": "2025-04-21T15:36:33.302263Z",
     "shell.execute_reply.started": "2025-04-21T15:36:33.291368Z"
    }
   },
   "outputs": [],
   "source": [
    "def readfile(path, file):\n",
    "    file_path = os.path.join(path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "def reduce_mem_usage(df, verbose=True, convert_obj_to_category=True, skip_cols=[]):\n",
    "    \"\"\"\n",
    "    Reduces memory usage of a DataFrame by downcasting numeric types\n",
    "    and optionally converting object types to categorical.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - verbose: print memory usage stats\n",
    "    - convert_obj_to_category: convert object columns to category if cardinality is low\n",
    "    - skip_cols: list of column names to skip during optimization\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in skip_cols:\n",
    "            continue\n",
    "\n",
    "        col_data = df[col]\n",
    "        col_type = col_data.dtype\n",
    "\n",
    "        try:\n",
    "            if is_numeric_dtype(col_data):\n",
    "                c_min = col_data.min()\n",
    "                c_max = col_data.max()\n",
    "\n",
    "                if is_integer_dtype(col_data):\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                        df[col] = col_data.astype(np.int8)\n",
    "                    elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                        df[col] = col_data.astype(np.int16)\n",
    "                    elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                        df[col] = col_data.astype(np.int32)\n",
    "                    else:\n",
    "                        df[col] = col_data.astype(np.int64)\n",
    "                elif is_float_dtype(col_data):\n",
    "                    if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                        df[col] = col_data.astype(np.float16)\n",
    "                    elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                        df[col] = col_data.astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = col_data.astype(np.float64)\n",
    "\n",
    "            elif convert_obj_to_category and is_object_dtype(col_data):\n",
    "                num_unique_values = col_data.nunique()\n",
    "                num_total_values = len(col_data)\n",
    "                if num_unique_values / num_total_values < 0.5:\n",
    "                    df[col] = col_data.astype('category')\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Could not process column {col}: {e}\")\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory usage decreased to {end_mem:5.2f} MB \"\n",
    "              f\"({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:36:43.584821Z",
     "iopub.status.busy": "2025-04-21T15:36:43.584481Z",
     "iopub.status.idle": "2025-04-21T15:37:13.271860Z",
     "shell.execute_reply": "2025-04-21T15:37:13.270855Z",
     "shell.execute_reply.started": "2025-04-21T15:36:43.584793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144233, 41)\n",
      "(590540, 394)\n",
      "(590540, 434)\n",
      "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
      "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
      "       ...\n",
      "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
      "       'DeviceType', 'DeviceInfo'],\n",
      "      dtype='object', length=434)\n",
      "No Frauds 96.5 % of the dataset\n",
      "Frauds 3.5 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "path = r'/kaggle/input/thesis-dataset'\n",
    "\n",
    "\n",
    "identity_file_name = 'train_identity.csv'\n",
    "transaction_file_name = 'train_transaction.csv'\n",
    "\n",
    "df_identity = readfile(path, identity_file_name)\n",
    "print(df_identity.shape)\n",
    "\n",
    "df_trans = readfile(path, transaction_file_name)\n",
    "print(df_trans.shape)\n",
    "\n",
    "df = pd.merge(df_trans, df_identity,how = \"left\", on = ['TransactionID'])\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "del df_trans, df_identity\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(df['isFraud'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['isFraud'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:37:50.641175Z",
     "iopub.status.busy": "2025-04-21T15:37:50.640849Z",
     "iopub.status.idle": "2025-04-21T15:37:58.569553Z",
     "shell.execute_reply": "2025-04-21T15:37:58.568502Z",
     "shell.execute_reply.started": "2025-04-21T15:37:50.641148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased to 525.70 MB (79.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_r = reduce_mem_usage(df, convert_obj_to_category=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:50:09.616865Z",
     "iopub.status.busy": "2025-04-21T15:50:09.616149Z",
     "iopub.status.idle": "2025-04-21T15:50:10.856750Z",
     "shell.execute_reply": "2025-04-21T15:50:10.855669Z",
     "shell.execute_reply.started": "2025-04-21T15:50:09.616826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_24    0.991962\n",
      "id_25    0.991310\n",
      "id_07    0.991271\n",
      "id_08    0.991271\n",
      "id_21    0.991264\n",
      "id_26    0.991257\n",
      "id_22    0.991247\n",
      "id_23    0.991247\n",
      "id_27    0.991247\n",
      "dist2    0.936284\n",
      "D7       0.934099\n",
      "id_18    0.923607\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing ratio\n",
    "missing_ratio = df_r.isnull().mean()\n",
    "\n",
    "# Filter columns with more than 95% missing\n",
    "high_missing_cols = missing_ratio[missing_ratio > 0.90].sort_values(ascending=False)\n",
    "\n",
    "# Show them\n",
    "print(high_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:14.141344Z",
     "iopub.status.busy": "2025-04-21T15:39:14.140940Z",
     "iopub.status.idle": "2025-04-21T15:39:14.148346Z",
     "shell.execute_reply": "2025-04-21T15:39:14.147259Z",
     "shell.execute_reply.started": "2025-04-21T15:39:14.141314Z"
    }
   },
   "outputs": [],
   "source": [
    "categoric_columns = ['ProductCD',\n",
    "                     'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "                     'addr1', 'addr2',\n",
    "                     'P_emaildomain', 'R_emaildomain',\n",
    "                     'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "                     'DeviceType', 'DeviceInfo',\n",
    "                     'id_12', 'id_13', 'id_14', 'id_15', 'id_16','id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24',\n",
    "                     'id_25', 'id_26','id_27','id_28','id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34','id_35', 'id_36', 'id_37', 'id_38']\n",
    "numeric_columns = []\n",
    "\n",
    "for i in df_r.columns:\n",
    "    if i not in categoric_columns:\n",
    "        numeric_columns.append(i)\n",
    "\n",
    "#print('\\n Numerical columns: ', numeric_columns)\n",
    "#print('\\n Categoric columns: ', categoric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:48.745164Z",
     "iopub.status.busy": "2025-04-21T15:39:48.744856Z",
     "iopub.status.idle": "2025-04-21T15:39:53.005563Z",
     "shell.execute_reply": "2025-04-21T15:39:53.004694Z",
     "shell.execute_reply.started": "2025-04-21T15:39:48.745139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 414 columns in train dataset with missing values.\n",
      "(590540, 202)\n",
      "232\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of null values in each column\n",
    "null_percentage = df_r.isnull().mean() * 100\n",
    "\n",
    "# Set a threshold for the null percentage\n",
    "threshold = 10  \n",
    "\n",
    "# Filter the columns based on the null percentage threshold\n",
    "filtered_columns = null_percentage[null_percentage > threshold].index\n",
    "#print(filtered_columns)\n",
    "\n",
    "\n",
    "# Create a new DataFrame with the filtered columns\n",
    "df_v1 = df_r[filtered_columns]\n",
    "\n",
    "for i in filtered_columns:\n",
    "    if i in categoric_columns:\n",
    "        categoric_columns.remove(i)\n",
    "    else:\n",
    "        if i in numeric_columns:\n",
    "            numeric_columns.remove(i)\n",
    "            \n",
    "#print('Numerical columns: ', numeric_columns)\n",
    "#print('Categoric columns: ', categoric_columns)\n",
    "\n",
    "df_null = pd.DataFrame(df_r.isnull().sum())\n",
    "df_null.reset_index(inplace = True)\n",
    "df_null.columns = ['Column','Nulls']\n",
    "df_null['percent']=df_null['Nulls'].apply(lambda x: (x/len(df_r)) * 100)\n",
    "\n",
    "print(f'There are {df_r.isnull().any().sum()} columns in train dataset with missing values.')\n",
    "\n",
    "count_null = 0\n",
    "\n",
    "null_30 = []\n",
    "\n",
    "for i in range(len(df_null)):\n",
    "    if df_null.iloc[i,2] >30:\n",
    "        count_null += 1\n",
    "        null_30.append(df_null.iloc[i,0])\n",
    "        \n",
    "\n",
    "#print(null_30)\n",
    "\n",
    "df_1 = df_r.drop(columns = null_30, axis = 1)\n",
    "\n",
    "print(df_1.shape)\n",
    "\n",
    "print(len(null_30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['TransactionDT']= pd.to_datetime(df_1['TransactionDT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T08:56:51.798401Z",
     "iopub.status.busy": "2025-04-20T08:56:51.798122Z",
     "iopub.status.idle": "2025-04-20T08:56:51.817162Z",
     "shell.execute_reply": "2025-04-20T08:56:51.816517Z",
     "shell.execute_reply.started": "2025-04-20T08:56:51.798383Z"
    }
   },
   "source": [
    "df_1['TransactionDT'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T08:56:55.009250Z",
     "iopub.status.busy": "2025-04-20T08:56:55.008525Z",
     "iopub.status.idle": "2025-04-20T08:56:55.013297Z",
     "shell.execute_reply": "2025-04-20T08:56:55.012488Z",
     "shell.execute_reply.started": "2025-04-20T08:56:55.009227Z"
    }
   },
   "source": [
    "print('Numerical columns: ', numeric_columns)\n",
    "print('Categoric columns: ', categoric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify categorical and numerical columns\n",
    "categorical_cols = df_1.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df_1.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "\n",
    "# Step 2: Impute missing values\n",
    "# Numeric: Replace NaN with 0\n",
    "df_1[numerical_cols] = df_1[numerical_cols].fillna(0)\n",
    "\n",
    "# Categorical: Replace NaN with 'NA'\n",
    "df_1[categorical_cols] = df_1[categorical_cols].fillna('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T08:57:12.260989Z",
     "iopub.status.busy": "2025-04-20T08:57:12.260727Z",
     "iopub.status.idle": "2025-04-20T08:57:12.725126Z",
     "shell.execute_reply": "2025-04-20T08:57:12.724434Z",
     "shell.execute_reply.started": "2025-04-20T08:57:12.260969Z"
    }
   },
   "source": [
    "df_1.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2_v1 = reduce_mem_usage(df_1, convert_obj_to_category=True)\n",
    "\n",
    "df_2 = df_2_v1.copy()\n",
    "\n",
    "\n",
    "df_2.drop(columns=['TransactionID','TransactionDT'], inplace = True )\n",
    "\n",
    "df_2['isFraud'] = df_2['isFraud'].astype('category')\n",
    "\n",
    "categorical_cols = df_2.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df_2.select_dtypes(include=['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns.tolist()\n",
    "#numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "df_2[numerical_cols] = scaler.fit_transform(df_2[numerical_cols])\n",
    "\n",
    "#df_2.head()\n",
    "\n",
    "cat_onehot=[]\n",
    "cat_label = []\n",
    "\n",
    "for i in categorical_cols:\n",
    "    if i != 'isFraud':\n",
    "        if df_2[i].nunique()<=10:\n",
    "            cat_onehot.append(i)\n",
    "        else:\n",
    "            cat_label.append(i)\n",
    "\n",
    "#print(cat_onehot)\n",
    "#print(cat_label)\n",
    "\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_label:\n",
    "    le = LabelEncoder()\n",
    "    df_2[col + '_LE'] = le.fit_transform(df_2[col].astype(str))\n",
    "    label_encoders[col] = le  # Store encoder if you need to inverse transform later\n",
    "\n",
    "#print(df_2.head())\n",
    "\n",
    "# One-Hot Encoding using scikit-learn\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop=None)\n",
    "\n",
    "encoded_array = onehot_encoder.fit_transform(df_2[cat_onehot])\n",
    "\n",
    "# Convert to DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=onehot_encoder.get_feature_names_out(cat_onehot))\n",
    "\n",
    "# Combine with original DataFrame\n",
    "df_encoded = pd.concat([df_2, encoded_df], axis=1)\n",
    "\n",
    "df_encoded.drop(columns =['ProductCD', 'card4', 'card6', 'P_emaildomain', 'M6'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "#print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop('isFraud', axis=1)\n",
    "y = df_encoded['isFraud']\n",
    "\n",
    "# Optional: split before SMOTE to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "X_res_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "y_res_df = pd.Series(y_resampled, name='isFraud')\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(y_res_df.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T08:58:21.211864Z",
     "iopub.status.busy": "2025-04-20T08:58:21.211551Z",
     "iopub.status.idle": "2025-04-20T09:02:07.814144Z",
     "shell.execute_reply": "2025-04-20T09:02:07.813346Z",
     "shell.execute_reply.started": "2025-04-20T08:58:21.211840Z"
    }
   },
   "source": [
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "lgbm.fit(\n",
    "    X_res_df, y_res_df,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(100)]\n",
    ")\n",
    "\n",
    "lgb_pred = lgbm.predict_proba(X_test)[:, 1]\n",
    "print(\"LightGBM AUC:\", roc_auc_score(y_test, lgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T09:02:24.233518Z",
     "iopub.status.busy": "2025-04-20T09:02:24.232805Z",
     "iopub.status.idle": "2025-04-20T09:05:17.871783Z",
     "shell.execute_reply": "2025-04-20T09:05:17.870977Z",
     "shell.execute_reply.started": "2025-04-20T09:02:24.233492Z"
    }
   },
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "xgb_pred = xgb_model.predict_proba(X_test)[:, 1]\n",
    "print(\"XGBoost AUC:\", roc_auc_score(y_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FraudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = FraudDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, heads):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "class FraudTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=64, heads=4, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim, heads) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Sequential(\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(embed_dim * 1, 64),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(64, 1)  # No sigmoid\n",
    "                            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # [B, 1, D]\n",
    "        x = self.blocks(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = FraudTransformer(input_dim=X.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)  # âœ… Apply sigmoid here\n",
    "            val_preds.extend(probs.cpu().numpy())\n",
    "            val_true.extend(yb.numpy())\n",
    "\n",
    "    auc = roc_auc_score(val_true, val_preds)\n",
    "    print(f\"Epoch {epoch+1}: AUC = {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7196903,
     "sourceId": 11482852,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
